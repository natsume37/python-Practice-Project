# python并发编程

## 多道技术

-   目的：单核实现并发效果
-   并发：看起来像同时运xing
-   并行：真正意义的同时执行
-   单核不能实现并行、但可实现并发
-   多道：允许多个应用程序同时进入内存、并且CPU可以交替运行



## 进程

-   程序：存在硬盘上的一堆代码、死的
-   进程：表示程序正在执行的过程、活的



## 进程调度

-   先来先服务调度算法
-   短作业优先调度算法
-   时间片轮+多级反馈队列
    -   越往下、任务的优先级越低
    -   越往下、任务的耗时越长



### 进程的三状态图

![进程三状态图](https://natsume-1316988601.cos.ap-chengdu.myqcloud.com/image_own/20251001150129230.png)

## 同步和异步

用来描述任务的提交方式

-   同步：任务提交后、一直原地等待任务的处理结果、等待过程中不做任何事
-   异步：任务提交后、不在原地等待、而是直接做其它事



## 阻塞和非阻塞

描述进程的运行状态

-   阻塞：阻塞态
-   非阻塞：就绪态、运行态



## 创建进程

```python
"""
创建进程的方法
os.fork()  :调用底层的fork()方法、但是Windows不支持
multiprocessing :主要用
subprocess：功能少点、运维用的多
"""
# 方法一
from multiprocessing import Process
import time


def func(name):
    print(f"{name}任务开始")
    time.sleep(5)
    print(f"{name}任务结束")

# windows必须加这个、要不然报错
if __name__ == '__main__':
    """
    windows创建子进程的方式是导入模块的模式、所以牵扯循环导入的问题。
    """
    # 1、进程操作对象
    p = Process(target=func, args=("子线程",))
    # 2、创建进程
    p.start()
    print("主进程")

# 方法2
from multiprocessing import Process
import time


class MyProcess(Process):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        print(f"{self.name}开始")
        time.sleep(5)
        print(f"{self.name}结束")


if __name__ == '__main__':
    p = MyProcess("子进程")
    p.start()
    print("主进程开始")
```

总结：创建进程就相当于在内存里申请了一块内存空间、然后把需要允许的代码放进去、多个进程的内存空间是相互隔离的。进程之间没办法直接进行数据交互、如果想要交互需要借助第三方工具/模块

>   windows 系统创建进程时需要在==\_\_main\_\_==方法下（windows创建进程的本质是模块导入的方式、二mac/linux是copy的方式



## join方法

用于等待进程执行完毕后才开始继续执行join方法后的主线程的代码

```python
from multiprocessing import Process
import time


def func(name, n):
    print(f"{name}开始执行")
    time.sleep(n)
    print(f"{name}开始执行")


if __name__ == '__main__':
    start = time.time()
    # p1 = Process(target=func, args=("1写论文", 1))
    # p2 = Process(target=func, args=("2写论文", 2))
    # p3 = Process(target=func, args=("3写论文", 3))
    # p1.start()
    # p2.start()
    # p3.start()
    l = []
    for i in range(1,4):
        p = Process(target=func, args=(f"写论文{i}", i))
        p.start()
        l.append(p)
    for p in l:
        p.join()
    print("主进程开始执行")
    end = time.time()
    print(end - start)
"""
代码无法直接创建进程、都是通知操作系统完成、所以顺序随机。
"""
```

## 进程间的数据隔离

在各进程间数据是隔离的、每一个进程在拿到资源后都会拷贝一份、所以多个进程之间的数据不交互。

```python
from multiprocessing import Process

age = 18

def func():
    global age
    age = 16

if __name__ == '__main__':
    p = Process(target=func)
    p.start()
    p.join()
    print(age)
    # 18 :主进程并没有被修改
```

## 进程号

```python
# pid号（进程号）：同一计算机、独一无二
# windows 查询进程号、tasklist|findstr <进程号>
from multiprocessing import Process, current_process
import time
import os


def task(name):
    # print(f"{current_process().pid}任务打印中") # 获取当前进程号pid
    print(f"{name}的进程号{os.getpid()}")  # 获取当前进程的pid
    print(f"{name}的父进程号{os.getppid()}")  # 获取父进程的pid
    time.sleep(100)


if __name__ == '__main__':
    p = Process(target=task, args=("子进程",))
    p.start()
    p.terminate()  # 杀死进程  Windows：taskkill  <pid> ,mac\linux  kill -9 <pid>
    time.sleep(0.001)
    print(p.is_alive())  # 打印进程存活状态  代码速度太快、可能操作系统还没来得及杀死进程
    print(f"主进程：{current_process().pid}")
    task("主进程")


"""
主进程：13088
主进程的进程号13088
主进程的父进程号6072  # pycharm进程
子进程的进程号3872
子进程的父进程号13088
"""
```

僵尸进程和孤儿进程

-   僵尸进程
    -   子进程死后还是会占用一些资源（进程号、进程的运行状态、运行时间等），等待父进程通过操作系统的调用回收（收尸）
    -   除了init进程之外，所有的进程最后都会步入僵尸进程
    -   危害
        -   子进程退出后、父进程没有及时处理、僵尸进程会一直占用计算机资源
        -   如果产生大量的僵尸进程、资源会被过度占用、系统没有可用的进程号、导致系统没法产生新的进程
-   孤儿进程
    -   子进程处于存活状态但是父进程以外死了
    -   操作系统会开设一个孤儿院“init进程”，用来管理孤儿进程，回收孤儿进程的相关资源

## 守护进程

```python
# daemon = True

import time
# 守护进程 ：主进程活、子进程活、主进程死、守护进程也必须死

from multiprocessing import Process


def task(name):
    print(f"{name}还活着")
    time.sleep(3)
    print(name, "killed")


if __name__ == '__main__':
    p = Process(target=task, kwargs={'name': "妲己"})
    p.daemon = True  # 设置守护进程、必须在启动进程前设置！！！
    p.start()
    time.sleep(1)
    print("主进程死亡")
```

主进程结束后立即结束守护进程（主进程活、守护进程活。主进程死，守护进程也立马死）



## 互斥锁

当多个进程操作同一份数据时，会出现数据混乱的问题，需要加锁处理

**把并发改为串行、虽然牺牲了运行效率但是保证了数据安全**

```python 
import json
import random
from multiprocessing import Process, Lock

import time


# 查询车票
def search_ticket(name):
    with open('data/tickets.json', 'r', encoding='utf-8') as f:
        dic = json.load(f)
        print(f"用户{name}查询余票{dic.get("tickets_num")}")


# 买票
def buy_ticket(name):
    # 查询车票
    with open('data/tickets.json', 'r', encoding='utf-8') as f:
        dic = json.load(f)
    # 模拟网络延时
    time.sleep(random.randint(1, 5))
    if dic.get("tickets_num") > 0:
        dic["tickets_num"] -= 1
        with open('data/tickets.json', 'w', encoding='utf-8') as f:
            json.dump(dic, f)
        print(f"用户{name}买票成功")
    else:
        print(f"余票不足、用户{name}买票失败")


def task(name, mutex):
    search_ticket(name)
    # 抢锁  获得
    mutex.acquire()
    buy_ticket(name)
    # 释放锁  释放
    mutex.release()


if __name__ == '__main__':
    mutex = Lock()
    for i in range(1, 8):
        p = Process(target=task, args=(i, mutex))
        p.start()
```



## 消息队列

-   队列：先进先出
    -   管道+锁
-   堆栈：先进后出

```python
# import queue
#
# queue.Queue()
from multiprocessing import Queue

q = Queue(6)  # 有默认大小
q.put("a")
q.put("b")
q.put("c")
q.put("d")
q.put("e")
q.put("e")
q.full()  # 判断队列是否已满？bool
# q.put("f",timeout=3)
# q.put_nowait("f") # 不等待、直接报错
# q.put("g")  # 多存会阻塞
v1 = q.get()
v2 = q.get()
v3 = q.get()
v4 = q.get()
v5 = q.get()
v6 = q.get()
q.empty()  # 判断是否拿完？ 空
# v7 = q.get()
# v7 = q.get_nowait()
# v7 = q.get(timeout=3)
print(v1)

"""
q.put()
q.get()

# 以下方法多进程可能不准确
q.put_nowait()
q.get_nowait()
q.full()
q.empty()
"""
```

## 进程间的通讯（IPC机制）

```python
from multiprocessing import Queue, Process


def task1(q):
    q.put("宫保鸡丁")


def task2(q):
    print(q.get())


if __name__ == '__main__':
    q = Queue()
    p1 = Process(target=task1, args=(q,))
    p2 = Process(target=task2, args=(q,))
    p1.start()
    p2.start()
    # p.join()  # 不需要 拿不到数据会阻塞
```

## 生产者消费者模型

生产者：（厨子）：生成或者制造数据的

消费者：（顾客）：消费或者处理数据的

媒介（桌子）：消息队列

```python
from multiprocessing import Process, Queue
import time
import random


def producer(name, food, q):
    for i in range(8):
        time.sleep(random.randint(1, 3))
        print(f"{name}做了{food}")
        q.put(f"{food}{i}")


def consumer(name, q):
    while True:
        food = q.get()
        time.sleep(random.randint(1, 3))
        print(f"{name}吃了{food}")
        if food == "鹤顶红":
            break


if __name__ == '__main__':
    q = Queue()
    p1 = Process(target=producer, args=("Martin", "黄金蛋炒饭", q))
    p2 = Process(target=producer, args=("神厨小福贵", "佛跳墙", q))
    c1 = Process(target=consumer, args=("八戒", q))
    c2 = Process(target=consumer, args=("悟空", q))
    p1.start()
    p2.start()
    c1.start()
    c2.start()

    # 等待生产者全部生产完、然后准备结束
    p1.join()
    p2.join()
    q.put("鹤顶红")
    # 因为有两个消费者所以要加两个毒
    q.put("鹤顶红")


# 统计消费者、智能添加鹤顶红（元类方法统计消费者个数）
from multiprocessing import Process, Queue
import time
import random
from multiprocessing.dummy import JoinableQueue


# 定义元类
class CountMeta(type):
    _count = 0

    def __call__(cls, *args, **kwargs):
        # 每实例化一次，就加 1
        CountMeta._count += 1
        return super().__call__(*args, **kwargs)

    @classmethod
    def get_count(mcls):
        return mcls._count


# 定义一个使用元类的 Process 子类
class MyProcess(Process, metaclass=CountMeta):
    pass


def producer(name, food, q):
    for i in range(8):
        time.sleep(random.randint(1, 3))
        print(f"{name}做了{food}")
        q.put(f"{food}{i}")


def consumer(name, q):
    while True:
        food = q.get()
        time.sleep(random.randint(1, 3))
        print(f"{name}吃了{food}")
        if food == "鹤顶红":
            break


if __name__ == '__main__':
    q = JoinableQueue()
    p1 = MyProcess(target=producer, args=("Martin", "黄金蛋炒饭", q))
    p2 = MyProcess(target=producer, args=("神厨小福贵", "佛跳墙", q))
    c1 = Process(target=consumer, args=("八戒", q))
    c2 = Process(target=consumer, args=("悟空", q))
    p1.start()
    p2.start()
    c1.start()
    c2.start()
    p1.join()
    p2.join()
```

```python
# JoinableQueue 可等待队列
from multiprocessing import Process, Queue, JoinableQueue
import time
import random

"""
JoinableQueue 在queue的基础上多了个计数器机制、没put一个数据、计数器+1
调用一次task_done()计数器-1
计数器为0时、走q.join()后的代码
"""


def producer(name, food, q):
    for i in range(8):
        time.sleep(random.randint(1, 3))
        print(f"{name}做了{food}")
        q.put(f"{food}{i}")


def consumer(name, q):
    while True:
        food = q.get()
        time.sleep(random.randint(1, 3))
        print(f"{name}吃了{food}")
        # if food == "鹤顶红":
        #     break
        q.task_done()  # 告诉队列、已经拿走了一个数据且已经处理完了


if __name__ == '__main__':
    q = JoinableQueue()
    p1 = Process(target=producer, args=("Martin", "黄金蛋炒饭", q))
    p2 = Process(target=producer, args=("神厨小福贵", "佛跳墙", q))
    c1 = Process(target=consumer, args=("八戒", q))
    c2 = Process(target=consumer, args=("悟空", q))
    p1.start()
    p2.start()

    # 设置守护进程、保证子进程死掉
    c1.daemon = True
    c2.daemon = True

    c1.start()
    c2.start()

    # 等待生产者全部生产完、然后准备结束
    p1.join()
    p2.join()

    q.join()
    # 主进程死了、消费者也要死掉（守护进程）
```

## 线程

进程：资源单位

线程：执行单位



创建进程

-   申请内存空间 消耗资源
-   拷贝代码   消耗资源



创建线程：在同一个进程下可以创建多个线程，同一进程内线程多个线程的数据是共享的

-   不需要再次申请内存空间
-   不需要拷贝代码

## 创建线程

```python
from multiprocessing import Process
from threading import Thread
import time

# # 创建线程方法1
# def task(name):
#     print(f"{name} 任务开始")
#     time.sleep(3)
#     print(f"{name} 任务结束")
#
#
# if __name__ == '__main__':
#     t = Thread(target=task, args=("悟空",))
#     t.start()
#     print("主线程")
"""
线程不需要申请内存空间、不需要拷贝代码、资源消耗比创建进程少很多、可以不用写在__main__下
"""


class MyThread(Thread):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        print(f"{self.name} 任务开始")
        time.sleep(3)
        print(f"{self.name} 任务结束")


if __name__ == '__main__':
    t = MyThread("悟空")
    t.start()
    print("主线程")

"""
t.join()
"""
```

## TCP 并发效果

```python
# TCP服务端代码
import socket
from multiprocessing import Process
from threading import Thread


def task(conn):
    while True:
        try:
            data = conn.recv(1024)
        except:
            break
        if not data:
            break
        print(data.decode("utf-8"))
        conn.send(data.upper())
    conn.close()


if __name__ == '__main__':
    # 默认tcp协议
    sk = socket.socket()
    sk.bind(("127.0.0.1", 8081))
    sk.listen(5)

    while True:
        print("服务器开启.....")
        conn, addr = sk.accept()
        p = Process(target=task, args=(conn,))
        p.start()
```

## 进程内数据共享

```python
from threading import Thread, current_thread, active_count
import os
"""
current_thread().name # 打印当前线程的名字（随机名）
active_count()： 活跃的线程数量（注意执行速度）
"""
age = 18


def task():
    print("子线程", os.getpid())
    global age
    age = 16


if __name__ == '__main__':
    t = Thread(target=task)
    t.start()
    # print("主线程", os.getpid())
    t.join()  # 保证子线程先执行
    print(age)
```

## 守护线程

```python
from threading import Thread
import time


def task(name):
    print(f"{name} 还活着")
    time.sleep(3)
    print(f"{name} 正常死亡")


if __name__ == '__main__':
    t = Thread(target=task, args=("妲己",))
    t.daemon = True
    t.start()
    print("纣王驾崩了")
# 主线程运行完毕后不会立即结束、而是等待所有子线程都结束后才结束
# 主线程结束意味着主线程所在的主进程结束了
```

## GIL全局解释器锁

https://wiki.python.org/moin/GlobalInterpreterLock

In CPython, the **global interpreter lock**, or **GIL**, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. The GIL prevents race conditions and ensures thread safety. A nice explanation of [how the Python GIL helps in these areas can be found here](https://python.land/python-concurrency/the-python-gil). In short, this mutex is necessary mainly because CPython's memory management is not thread-safe.

在Cpython解释器中GIL是一把互斥锁，用来阻止同一个进程下的多个线程同时执行，也就是说在同一进程下的多个线程，它们没办法并行，有多个CPU都不能并行，以此只能有一个CPU来执行

~~问题：python的多线程好像没什么卵用？无法利用多核的优势，即使有多核、一次也只能用一个~~

分情况：

-   单核
    -   10个任务（计算密集型、I/O密集型）
-   多核
    -   10个任务（计算密集型、I/O密集型）

##### 多核

-   计算密集型  每个任务10s,cpu10核
    -   多线程 100+
    -   多进程 10s+  **效率高**
-   IO密集型
    -   多进程  **节省资源**
    -   多线程  浪费资源

总结：

-   多进程核多线程都有各自的优势。
-   现在开发的程序90%以上都是IO密集型的，多线程优势更大
-   多进程使用场景（挖矿、造氢弹原子弹、训练人工智能...)，利用CPU多核能力

Cpython的线程管理不是线程安全的

![GIL示意图](https://natsume-1316988601.cos.ap-chengdu.myqcloud.com/image_own/20251001160707861.png)

##### 内存管理（垃圾回收机制）

-   引用计数
-   标记清除
-   分代回收

GC（Garbage cleaning）巡逻

​	垃圾回收扫描、第一代正常巡逻、第二代会给相应的权重、越往下巡逻的频率越少（减少常驻内存变量的多次巡逻的资源占用）因为GC也会占用系统资源

##### python解释器

-   **Cpython**
-   Jpython
-   Pypypython

注意：

-   GIL不是python的特点，而是Cpython独有的特点。
-   GIL会导致同一个进程下的多个线程不能同时运行，无法利用多核优势
-   GIL保证的是解释器级别的数据的安全
-   写代码的时候该怎么写怎么写、不用考虑GIL

```python
# 验证
from threading import Thread, Lock
import time

num = 180
mutex = Lock()


def task():
    global num
    # mutex.acquire()
    with mutex: # 另一种写法
        temp = num
        # 通过注释掉模拟延迟，来验证GIL
        time.sleep(0.5)
        num = temp - 1
    # mutex.release()


if __name__ == '__main__':
    l = []
    for i in range(180):
        t = Thread(target=task)
        t.start()
        l.append(t)

    for t in l:
        t.join()
    print(num)
# 结果：0
```

##### 为什么加延迟数据会错乱？

由于每个线程都要抢GIL锁、第一个线程抢到后遇到I/O操作需要释放GIL锁、让其他线程抢GIL锁，在0.05秒内180个线程都执行到temp=num这一步。此时所有的线程num的值都为180、然后一个个执行-1操作

但正常情况下网络延迟是没办法避免的所以我们要自己加锁处理保证数据安全。

加锁情况

```python 
第一个线程抢GIL锁 -> 获取mutex锁 -> 获取num的值 —> 遇到I/O操作,GIL锁释放
第二个线程抢到GIL锁 —> 执行到 with mutex后遇到阻塞  —> 是否GIL锁  ...
180个线程都抢到了GIL锁  -> 第一个线程 I/O结束 -> GIL锁回到第一个线程继续执行
```

总结 ：虽然每个线程都抢到了GIL锁但是还是卡在了我们自定义的mutex互斥锁这里。

## 死锁

```python
# 死锁
from threading import Thread, Lock, current_thread
import time

mutex1 = Lock()
mutex2 = Lock()


def task():
    mutex1.acquire()
    print(current_thread().name, "抢到锁1")
    mutex2.acquire()
    print("抢到锁2")
    mutex2.release()
    mutex1.release()
    task2()


def task2():
    mutex2.acquire()
    print(current_thread().name, "抢到锁2")
    time.sleep(1)
    mutex1.acquire()
    print("抢到锁1")
    mutex1.release()
    mutex2.release()


if __name__ == '__main__':
    for i in range(8):
        t = Thread(target=task)
        t.start()
"""
Thread-1 (task) 抢到锁1
抢到锁2
Thread-1 (task) 抢到锁2
Thread-2 (task) 抢到锁1

阻塞了
"""
```

用互斥锁的时候谨慎使用



## 递归锁

递归锁内部有一个计数器、每acquire一次计数器+1（可以acquire多次）、每release一次计数器-1（可以release多次）直到计数器为0时其它人才能再次抢这把锁。

```python
from threading import Thread, RLock, current_thread
import time

# mutex1 等于 mutex2
mutex1 = mutex2 = RLock()


def task():
    mutex1.acquire()
    print(current_thread().name, "抢到锁1")
    mutex2.acquire()
    print("抢到锁2")
    mutex2.release()
    mutex1.release()
    task2()


def task2():
    mutex2.acquire()
    print(current_thread().name, "抢到锁2")
    time.sleep(1)
    mutex1.acquire()
    print("抢到锁1")
    mutex1.release()
    mutex2.release()


if __name__ == '__main__':
    for i in range(8):
        t = Thread(target=task)
        t.start()
```

## 信号量

信号量在不同阶段对应不同的技术点，对于并发编程来说他是“锁”

它可以用来控制同时访问特定资源的线程数量、通常对于某些特定资源有明确访问量限制的场景（限流）

例子：停车场

```py
"""
互斥锁：停车场只有一个车位
信号量：停车场有多个车位
"""

import random
from threading import Thread, Semaphore
import time

sp = Semaphore(5)

def task(name):
    sp.acquire()
    print(name,"抢到锁")
    time.sleep(random.randint(2,5))
    sp.release()

if __name__ == '__main__':
    for i in range(25):
        t = Thread(target=task, args=(f'宝马{i+1}',))
        t.start()
```

## Even事件

接收信号后程序开始执行、未接收到信号、代码原地阻塞。

```python
from threading import Thread, Event
import time

event = Event()

def bus():
    print("公交车即将进站")
    time.sleep(3)
    # 发送信号
    event.set()
    print("公交车已到站")

def passenger(name):
    print(name,"wait")
    # 等待信号、没有信号原地阻塞
    event.wait()
    print(name,"上车出发")

if __name__ == '__main__':
    t = Thread(target=bus)
    t.start()
    for i in range(10):
        t2 = Thread(target=passenger, args=(f"乘客{i+1}",))
        t2.start()
```

## 池

池是用来在保证计算机硬件安全的情况下最大限度的利用计算机资源，降低了计算机的运行效率、但是保证了计算机的运行安全

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

# pool = ThreadPoolExecutor()  # 目前默认CPU个数加4
pool = ProcessPoolExecutor()  # 目前默认CPU个数加4


def task(name):
    print(name)
    time.sleep(3)
    return name + 10

if __name__ == '__main__':
    f_list = []
    for i in range(50):
        # pool.submit(task,i) # 往线程池提交任务 异步提交
        # print("主线程")
        future = pool.submit(task, i)
        f_list.append(future)



    pool.shutdown() # 关闭线程池、等待线程池中的线程全部执行完毕

    # 将异步提交的结果全部放到列表里然后同步打印
    for f in f_list:
        print(f"任务结果：{f.result()}")
```

## 回调机制

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

# pool = ThreadPoolExecutor()  # 目前默认CPU个数加4
pool = ProcessPoolExecutor()  # 目前默认CPU个数加4


def task(name):
    print(name)
    time.sleep(3)
    return name + 10


def call_back(res):
    print(f"coll_back:{res.result()}")


if __name__ == '__main__':
    # f_list = []
    for i in range(50):
        future = pool.submit(task, i).add_done_callback(call_back)  # 增加回调机制：异步任务执行完毕后、会将返回结果传给回到函数进行处理
        # f_list.append(future)

    pool.shutdown()  # 关闭线程池、等待线程池中的线程全部执行完毕

    # 将异步提交的结果全部放到列表里然后同步打印
    # for f in f_list:
    #     print(f"任务结果：{f.result()}")
```

## 协程

也可以称为微线程，它是一种用户态内的上下文切换技术、简单来说就是单线程下实现并发效果

```python
"""
进程：资源单位
线程：执行单位
协程：程序员人为创造出来的（切换+保存状态）
"""
```

